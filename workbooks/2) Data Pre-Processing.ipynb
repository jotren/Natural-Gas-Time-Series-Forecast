{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "711a283d-b6a3-45ce-af7b-b75da7f55be4",
   "metadata": {},
   "source": [
    "# Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28781683-aefc-4a3d-80f9-04cf8af0ccdd",
   "metadata": {},
   "source": [
    "Based on the previous analysis, it seems there is a linear regression model than can be built here where Demand it related to Temperature. I think it would be best to have an __ensemble__ algorithmn where:\n",
    "\n",
    "1) __Ransac Linear Regression__: Look at air temp vs Demand for given days of the year. Seeing as I have multiple grids I would take the median prediction which could be robust.\n",
    "2) __Machine Learning__: Use an ML model to tidy up the prediction and look for micro trends.\n",
    "\n",
    "In the LR model I would like to run a regression for every grid_id for each day of the year where y = demand and x = airTemperature_min. The result will look something like this: \n",
    "\n",
    "```python\n",
    "{1: {'52.00000_-0.50000': {'slope': -0.9802276889431906, 'intercept': 19.459949154023224}, #The first number represents the day of the year.\n",
    "    '52.00000_0.00000': {'slope': -0.9310295962378231, 'intercept': 19.345573208158548}, \n",
    "    '52.00000_0.50000': {'slope': -0.9317602754566605, 'intercept': 19.110430006500174}, \n",
    "    '51.50000_-0.50000': {'slope': -0.9569986102825089, 'intercept': 19.978916812381136}, \n",
    "    ...}\n",
    "```\n",
    "\n",
    "Where each grid and day will have a gradient and intercept value. In order to make a more robust regression I will use RANSAC that is good with respect to outliers. I will then run a prediction for each day in my test and training data and use the LR output as a proxy-prediction. These proxy-predictions will then be fed into the second step machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbb6978b-e753-4608-b960-a6ece78cfd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.append('C:/projects/python/time-series-analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c447b245-534f-464c-b623-e24147933d42",
   "metadata": {},
   "source": [
    "Filter the locations to only include GDUK_EAST_ANGLIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ffbfcb2-0812-4f91-a6f3-c9ab9d1ec7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_locations = pd.read_csv(\"../data/raw/network_gridlocations_&_population.csv\")\n",
    "demand_data = pd.read_csv(\"../data/raw/demand_data/GDUK_EAST_ANGLIA.csv\")\n",
    "location = \"GDUK EAST ANGLIA\"\n",
    "grid_locations = grid_locations[grid_locations['PLANT']==location]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1425c3-bd00-4d55-896e-c0a0b7181893",
   "metadata": {},
   "source": [
    "# Linaer Regression RANSAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e36309a-f26d-49df-a756-f86221a3f358",
   "metadata": {},
   "source": [
    "Now we need to create a function that will take these list of gridIDs and then run a linea regression of demand vs. air temperature. We do this for all the grids and then take the median value as an initial prediction. We import oure pre-built class that can already achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe73cbc-b97b-41ee-bfd5-18fb05369a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.util.Initial_Linear_Regression import InitialLinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0214dbe4-ad1b-4d88-ad75-606f5b4042cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demand Data Pivot:\n",
      "Weather Data for GRID ID 52.00000_-0.50000 Processed\n",
      "Weather Data for GRID ID 52.00000_0.00000 Processed\n",
      "Weather Data for GRID ID 52.00000_0.50000 Processed\n",
      "Weather Data for GRID ID 51.50000_-0.50000 Processed\n",
      "Weather Data for GRID ID 52.50000_0.00000 Processed\n",
      "Weather Data for GRID ID 52.00000_1.00000 Processed\n",
      "Weather Data for GRID ID 52.50000_1.50000 Processed\n",
      "Weather Data for GRID ID 51.50000_0.00000 Processed\n",
      "Weather Data for GRID ID 52.50000_0.50000 Processed\n",
      "Weather Data for GRID ID 51.50000_0.50000 Processed\n",
      "Weather Data for GRID ID 52.00000_1.50000 Processed\n",
      "Weather Data for GRID ID 52.50000_1.00000 Processed\n",
      "Weather Data for GRID ID 53.00000_1.00000 Processed\n",
      "Weather Data for GRID ID 53.00000_1.50000 Processed\n",
      "Weather Data for GRID ID 52.50000_-0.50000 Processed\n",
      "Weather Data for GRID ID 53.00000_0.50000 Processed\n",
      "Weather Data for GRID ID 51.50000_1.00000 Processed\n",
      "Weather Data for GRID ID 53.50000_-2.00000 Processed\n",
      "Weather Data for GRID ID 52.50000_-2.50000 Processed\n",
      "Weather Data for GRID ID 52.50000_-1.50000 Processed\n",
      "Weather Data for GRID ID 53.00000_-0.50000 Processed\n",
      "Weather Data for GRID ID 53.00000_-1.00000 Processed\n",
      "Weather Data for GRID ID 53.50000_-3.00000 Processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting models:  99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 363/365 [01:04<00:00,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid data or insufficient data points for day 362, GRID ID 52.50000_-0.50000\n",
      "No valid data or insufficient data points for day 363, GRID ID 52.50000_-0.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting models: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 365/365 [01:04<00:00,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid data or insufficient data points for day 364, GRID ID 52.50000_-0.50000\n",
      "No valid data or insufficient data points for day 365, GRID ID 52.50000_-0.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = InitialLinearRegression(grid_locations, demand_data, \"../data/raw/noaa\")\n",
    "model.preprocess_demand_data()\n",
    "model.load_and_preprocess_weather_data()\n",
    "model.fit_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75602067-97e7-4355-97dc-290fb8a836a9",
   "metadata": {},
   "source": [
    "### Data Pre-Processing\n",
    "\n",
    "First we need to process the data. Will use a standard scaler and first try and feed as much information as I can into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faa0ef47-90bb-425a-a25f-8a9bbe06a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def apply_prediction(row, model, grid_id):\n",
    "    try:\n",
    "        day_of_year = pd.to_datetime(row['date']).dayofyear\n",
    "        temperature = row['airTemperature_min']\n",
    "        return model.predict(grid_id, day_of_year, temperature)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with prediction for grid {grid_id} on day {day_of_year}: {e}\")\n",
    "        return np.nan  # Return NaN to handle errors gracefully\n",
    "\n",
    "def process_weather_data(grid_locations, demand_data, test_period, model, save_path=\"../data/processed/machine_learning\"):\n",
    "    dataframe_list = []\n",
    "\n",
    "    # Loop through each grid ID, load the data, rename columns, and append to list\n",
    "    for grid_id in grid_locations['GRID_ID']:\n",
    "        df = pd.read_csv(f\"../data/raw/noaa/{grid_id}.csv\")\n",
    "        df['date'] = pd.to_datetime(df['date'])  # Ensure 'date' is in datetime format\n",
    "        df['linear_prediction'] = df.apply(apply_prediction, axis=1, args=(model, grid_id))\n",
    "        df.rename(columns={col: f\"{col}_{grid_id.replace('.', '')}\" if col != 'date' else col for col in df.columns}, inplace=True)\n",
    "        dataframe_list.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames along columns\n",
    "    combined_df = pd.concat(dataframe_list, axis=1)\n",
    "    combined_df = combined_df.loc[:,~combined_df.columns.duplicated()]\n",
    "\n",
    "    # Convert 'date' to datetime and ensure no duplicated date columns\n",
    "    combined_df['date'] = pd.to_datetime(combined_df['date'].drop_duplicates())\n",
    "    \n",
    "    # Merge with demand data, ensuring both 'date' columns are datetime\n",
    "    demand_data['date'] = pd.to_datetime(demand_data['Applicable For'])\n",
    "    demand_data = demand_data[['date', 'Value']]\n",
    "    combined_df = pd.merge(combined_df, demand_data, on='date', how='left')\n",
    "\n",
    "    # Create new time-based features\n",
    "    combined_df['day_of_year'] = combined_df['date'].dt.dayofyear\n",
    "    combined_df['day_of_week'] = combined_df['date'].dt.dayofweek\n",
    "    combined_df['day_of_month'] = combined_df['date'].dt.day\n",
    "    combined_df['month'] = combined_df['date'].dt.month\n",
    "    combined_df['year'] = combined_df['date'].dt.year\n",
    "    combined_df.dropna(inplace=True)\n",
    "\n",
    "    # Convert all relevant columns to float\n",
    "    combined_df = combined_df.astype({col: 'float' for col in combined_df.columns if col not in ['date']})\n",
    "    \n",
    "    # Sort combined dataframe by date\n",
    "    combined_df = combined_df.sort_values('date')\n",
    "\n",
    "    # Manually split the data into training and testing sets based on the last 'test_period' days\n",
    "    split_point = combined_df.shape[0] - test_period\n",
    "    split_point_dates = combined_df.iloc[split_point:]['date'].tolist()  # Get split point dates\n",
    "\n",
    "    X = combined_df.drop(columns=['Value', 'date'])  # Assuming 'date' should also be dropped from features\n",
    "    y = combined_df['Value'].astype(float)  # Ensure target is also float\n",
    "    X_train, X_test = X.iloc[:split_point], X.iloc[split_point:]\n",
    "    y_train, y_test = y.iloc[:split_point], y.iloc[split_point:]\n",
    "\n",
    "    # Convert DataFrames to numpy arrays\n",
    "    processed_data = {\n",
    "        'X_train': X_train.to_numpy(),\n",
    "        'X_test': X_test.to_numpy(),\n",
    "        'y_train': y_train.to_numpy(),\n",
    "        'y_test': y_test.to_numpy(),\n",
    "        'split_point_dates': split_point_dates\n",
    "    }\n",
    "\n",
    "    # Save column names\n",
    "    column_names = X.columns.tolist()\n",
    "    with open(os.path.join(save_path, \"column_names.pkl\"), 'wb') as f:\n",
    "        pickle.dump(column_names, f)\n",
    "    \n",
    "    # Save the processed data as NumPy arrays\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    # combined_df.to_csv(os.path.join(save_path, \"processed_data.csv\"), index=False)\n",
    "    np.save(os.path.join(save_path, \"X_train.npy\"), X_train)\n",
    "    np.save(os.path.join(save_path, \"X_test.npy\"), X_test)\n",
    "    np.save(os.path.join(save_path, \"y_train.npy\"), y_train)\n",
    "    np.save(os.path.join(save_path, \"y_test.npy\"), y_test)\n",
    "    np.save(os.path.join(save_path, \"split_point_dates.npy\"), np.array(split_point_dates))\n",
    "    \n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a99fd6a-48d3-44f7-800b-bbd7c532749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, y_test_dates = process_weather_data(grid_locations, demand_data, 360, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67355829-1ad2-410d-9759-97c317619524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (time-series)",
   "language": "python",
   "name": "time-series"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
